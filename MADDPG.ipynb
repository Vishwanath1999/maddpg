{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MADDPG.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OPqkVTdGtman","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659848292672,"user_tz":-330,"elapsed":2940,"user":{"displayName":"Viswanathan Colab","userId":"08129842318858794853"}},"outputId":"da085c66-fffe-49a8-d96d-9f55e7678527"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import os\n","import datetime\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","os.chdir('/content/gdrive/My Drive/Reinforcement Learning/actor_critic/MADDPG')\n","# !ls"]},{"cell_type":"code","source":["# !pip install gym==0.10.5\n","# !pip install -e .\n","# !pip install torch==1.4.0"],"metadata":{"id":"oV3W8DiPt_uB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from maddpg import MADDPG\n","from buffer import MultiAgentReplayBuffer\n","from make_env import make_env\n","import time\n","from gym import wrappers"],"metadata":{"id":"x8g1dhWctwSM","executionInfo":{"status":"ok","timestamp":1659848294029,"user_tz":-330,"elapsed":1365,"user":{"displayName":"Viswanathan Colab","userId":"08129842318858794853"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def obs_list_to_state_vector(observation):\n","    state = np.array([])\n","    for obs in observation:\n","        state = np.concatenate([state, obs])\n","    return state"],"metadata":{"id":"NsloBFhWtyeM","executionInfo":{"status":"ok","timestamp":1659848294030,"user_tz":-330,"elapsed":4,"user":{"displayName":"Viswanathan Colab","userId":"08129842318858794853"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    #scenario = 'simple'\n","    scenario = 'simple_tag'\n","    env = make_env(scenario)\n","    n_agents = env.n\n","    actor_dims = []\n","    for i in range(n_agents):\n","        actor_dims.append(env.observation_space[i].shape[0])\n","    critic_dims = sum(actor_dims)\n","\n","    # action space is a list of arrays, assume each agent has same action space\n","    n_actions = env.action_space[0].n\n","    maddpg_agents = MADDPG(actor_dims, critic_dims, n_agents, n_actions, \n","                           fc1=64, fc2=64,  \n","                           alpha=0.01, beta=0.01, scenario=scenario,\n","                           chkpt_dir='tmp/maddpg/')\n","\n","    memory = MultiAgentReplayBuffer(1000000, critic_dims, actor_dims, \n","                        n_actions, n_agents, batch_size=1024)\n","\n","    PRINT_INTERVAL = 500\n","    N_GAMES = 50000\n","    MAX_STEPS = 25\n","    total_steps = 0\n","    score_history = []\n","    evaluate = False\n","    best_score = 0\n","\n","    if evaluate:\n","        maddpg_agents.load_checkpoint()\n","\n","    for i in range(N_GAMES):\n","        obs = env.reset()\n","        score = 0\n","        done = [False]*n_agents\n","        episode_step = 0\n","        while not any(done):\n","            if evaluate:\n","                env.render()\n","                #time.sleep(0.1) # to slow down the action for the video\n","            actions = maddpg_agents.choose_action(obs)\n","            obs_, reward, done, info = env.step(actions)\n","\n","            state = obs_list_to_state_vector(obs)\n","            state_ = obs_list_to_state_vector(obs_)\n","\n","            if episode_step >= MAX_STEPS:\n","                done = [True]*n_agents\n","\n","            memory.store_transition(obs, state, actions, reward, obs_, state_, done)\n","\n","            if total_steps % 100 == 0 and not evaluate:\n","                maddpg_agents.learn(memory)\n","\n","            obs = obs_\n","\n","            score += sum(reward)\n","            total_steps += 1\n","            episode_step += 1\n","\n","        score_history.append(score)\n","        avg_score = np.mean(score_history[-100:])\n","        if not evaluate:\n","            if avg_score > best_score:\n","                # maddpg_agents.save_checkpoint()\n","                best_score = avg_score\n","        if i % PRINT_INTERVAL == 0 and i > 0:\n","            print('episode', i, 'average score {:.1f}'.format(avg_score))"],"metadata":{"id":"YNmEe7stt07x"},"execution_count":null,"outputs":[]}]}